{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "      temperature=0.5,\n",
    "      model=\"gpt-4o\",\n",
    "      openai_api_version=\"2024-02-01\",\n",
    "      azure_deployment=\"gpt4o\",\n",
    "      max_tokens=700)\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",  # Your Azure deployment name for embeddings\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import FAISS\n",
    "# import faiss\n",
    "# import pickle\n",
    "# from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# index = faiss.read_index(\"./data/vector_data/faiss_index\")\n",
    "\n",
    "# with open(\"./data/vector_data/faiss_documents.pkl\", \"rb\") as f:\n",
    "#     docstore_dict = pickle.load(f)\n",
    "\n",
    "# vector_store_new = FAISS(\n",
    "#     embedding_function=embedding_model,\n",
    "#     index=index,\n",
    "#     docstore=InMemoryDocstore(docstore_dict),\n",
    "#     index_to_docstore_id=dict(zip(range(len(docstore_dict)), docstore_dict.keys()))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmitRoy\\AppData\\Local\\Temp\\ipykernel_28560\\760551883.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(persist_directory=\"./data/vector_data/chroma_db\", embedding_function=embedding_model)\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma(persist_directory=\"./data/vector_data/chroma_db\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmitRoy\\AppData\\Local\\Temp\\ipykernel_28560\\489748204.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = retriever.get_relevant_documents(\"Best policy for older people\")\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n",
    "    \n",
    "res = retriever.get_relevant_documents(\"Best policy for older people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"Extracts valid JSON from an LLM response, removing markdown formatting and extra text.\"\"\"\n",
    "    match = re.search(r\"```json\\n(.*?)\\n```\", response, re.DOTALL)\n",
    "    return match.group(1) if match else response.strip()\n",
    "\n",
    "def restructure_query(original_query: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in restructuring queries to retrieve data from a vector store. Your goal is to extract policy name, plan name, and query intent.\n",
    "\n",
    "    Given the query: \"{original_query}\"\n",
    "\n",
    "    Instructions:\n",
    "    - Extract explicitly mentioned policies and plans.\n",
    "    - For comparative queries, break them into structured subqueries covering benefits, waiting periods, and exclusions.\n",
    "    - If a policy or plan is missing, include all relevant policies (ADITYA_BIRLA, CARE, HDFC_ERGO) or plans.\n",
    "    - Be innovative: restructure, add, or subtract details as needed to generate meaningful subqueries.\n",
    "    - Follow the same structure but do not copy exact queries; create diverse and insightful subqueries.\n",
    "    - Return output strictly in JSON format.\n",
    "\n",
    "    Available Plans:\n",
    "    1. {{\"company\": \"ADITYA_BIRLA\", \"plan\": \"activ_one_max\"}}\n",
    "    2. {{\"company\": \"ADITYA_BIRLA\", \"plan\": \"activ_fit_plus\"}}\n",
    "    3. {{\"company\": \"CARE\", \"plan\": \"supreme\"}}\n",
    "    4. {{\"company\": \"CARE\", \"plan\": \"supreme_value_for_money\"}}\n",
    "    5. {{\"company\": \"HDFC_ERGO\", \"plan\": \"optima_secure\"}}\n",
    "    6. {{\"company\": \"HDFC_ERGO\", \"plan\": \"optima_super_secure\"}}\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    Query: \"Waiting period of PED in case of HDFC ERGO Optima Secure.\"\n",
    "    Output:\n",
    "    {{\"query1\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_secure\", \"query\": \"Waiting period of Preexisting Diseases\"}}}}\n",
    "\n",
    "    Query: \"Compare HDFC ERGO Optima Secure with Aditya Birla Activ One Max.\"\n",
    "    Output:\n",
    "    {{\n",
    "        \"query1\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_secure\", \"query\": \"Benefits covered\"}},\n",
    "        \"query2\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_secure\", \"query\": \"Waiting Period and Exclusions\"}},\n",
    "        \"query3\": {{\"policy\": \"ADITYA_BIRLA\", \"plan\": \"activ_one_max\", \"query\": \"Benefits covered\"}},\n",
    "        \"query4\": {{\"policy\": \"ADITYA_BIRLA\", \"plan\": \"activ_fit_plus\", \"query\": \"Waiting Period and Exclusions\"}}\n",
    "    }}\n",
    "\n",
    "    Query: \"Which policy is best for older people\"\n",
    "    Output:\n",
    "    {{\n",
    "        \"query1\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_secure\", \"query\": \"Guidelines for older people (>50 years)\"}},\n",
    "        \"query2\": {{\"policy\": \"ADITYA_BIRLA\", \"plan\": \"activ_fit_plus\", \"query\": \"Guidelines for older people (>50 years)\"}},\n",
    "        \"query3\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_super_secure\", \"query\": \"Guidelines for older people (>50 years)\"}},\n",
    "        \"query4\": {{\"policy\": \"CARE\", \"plan\": \"supreme\", \"query\": \"Guidelines for older people (>50 years)\"}},\n",
    "        \"query5\": {{\"policy\": \"CARE\", \"plan\": \"supreme_value_for_money\", \"query\": \"Guidelines for older people (>50 years)\"}},\n",
    "        \"query6\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_secure\", \"query\": \"Premium and affordability for senior citizens\"}},\n",
    "        \"query7\": {{\"policy\": \"ADITYA_BIRLA\", \"plan\": \"activ_fit_plus\", \"query\": \"Pre-existing condition coverage for seniors\"}},\n",
    "        \"query8\": {{\"policy\": \"CARE\", \"plan\": \"supreme\", \"query\": \"Lifetime renewability options for seniors\"}},\n",
    "        \"query9\": {{\"policy\": \"HDFC_ERGO\", \"plan\": \"optima_super_secure\", \"query\": \"Special benefits for elderly policyholders\"}},\n",
    "        \"query10\": {{\"policy\": \"ADITYA_BIRLA\", \"plan\": \"activ_one_max\", \"query\": \"Comparison of premium rates for senior citizens\"}}\n",
    "    }}\n",
    "\n",
    "    Output Format:\n",
    "    {{\"query1\": {{\"policy\": \"\", \"plan\": \"\", \"query\": \"\"}}}}\n",
    "    Strictly follow this output format.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    sub_queries_response = llm.predict(prompt)\n",
    "    try:\n",
    "        cleaned_response = clean_json_response(sub_queries_response)\n",
    "        return json.loads(cleaned_response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON from LLM!\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query = \"Waiting period of pre existing disease of all the plans\"\n",
    "# query = \"Which Plan is best for older people\"\n",
    "# res = restructure_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_doc(policy, plan, query):\n",
    "    filter_conditions = []\n",
    "    if policy:\n",
    "        filter_conditions.append({\"company\": {\"$eq\": policy}})\n",
    "    if plan:\n",
    "        filter_conditions.append({\"plan\": {\"$eq\": plan}})\n",
    "    \n",
    "    # Apply filter correctly\n",
    "    if len(filter_conditions) > 1:\n",
    "        combined_filter = {\"$and\": filter_conditions}\n",
    "    elif len(filter_conditions) == 1:\n",
    "        combined_filter = filter_conditions[0]  # Use single condition directly\n",
    "    else:\n",
    "        combined_filter = None  # No filter\n",
    "    \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5, \"filter\": combined_filter}\n",
    "    )\n",
    "    \n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "def get_relevant_docs(res):\n",
    "    docs = []\n",
    "    if res.get('query'):\n",
    "        docs.extend(get_relevant_doc(\n",
    "            policy=res.get('policy'), \n",
    "            plan=res.get('plan'), \n",
    "            query=res.get('query')\n",
    "        ))\n",
    "    else:\n",
    "        for v in res.values():\n",
    "            docs.extend(get_relevant_doc(\n",
    "                policy=v.get('policy'), \n",
    "                plan=v.get('plan'), \n",
    "                query=v.get('query')\n",
    "            ))\n",
    "\n",
    "    doc_entries = [\n",
    "        f'\"company\": {doc.metadata[\"company\"]} \\n \"plan\": {doc.metadata[\"plan\"]} \\n \"content\": {doc.page_content}' for doc in docs\n",
    "    ]\n",
    "\n",
    "    return doc_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_document_with_llm(query: str, retrieved_doc: str) -> str:\n",
    "    prompt = f\"\"\"You are a grader assessing relevance of a retrieved document to a user query. If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. Give a score in between 0 to 10 indicate whether the document is relevant to the question.\n",
    "\n",
    "    Instructions:\n",
    "    - Score the retrieved document as 10 if it is directly related to the query.\n",
    "    - Score the retrieved document as 0 if it is completely irrelevant to the query.\n",
    "    - For all other cases, assign a score between 1(mostly irrelevant) and 9(mostly relevant) based on relevance.\n",
    "    - In certain cases, direct information may not be available. For example, if a query concerns benefits or disadvantages for older people, the retrieved documents might only contain general policy information for this demographic. In such cases, these documents should be given a higher score(>5).\n",
    "\n",
    "    - Only return the score no resoning no extra explanation otherwise you will be highly penalized\n",
    "    \n",
    "    Here is the retrieved document: \\n\\n {retrieved_doc} \\n\\n Here is the user question: \\n\\n {query}\n",
    "    \"\"\"\n",
    "\n",
    "    res = llm.predict(prompt)\n",
    "\n",
    "    return res\n",
    "\n",
    "def refine_documents_with_llm(query, doc_entries):\n",
    "    new_doc_entries = []\n",
    "\n",
    "    for doc_entry in doc_entries:\n",
    "        res = refine_document_with_llm(query, doc_entry)\n",
    "        if int(res) >= 2:\n",
    "            new_doc_entries.append(doc_entry)\n",
    "        # else:\n",
    "        #     print(doc_entry)\n",
    "\n",
    "    new_doc_entries_str = \"\\n\\n\".join(new_doc_entries)\n",
    "    return new_doc_entries_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_content_refinement_tool(query, retrieved_docs):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in content refinement. Given a user query and relevant documents retrieved from the database, generate a well-structured and comprehensive response using all available information. Ensure clarity, conciseness, coherence, and factual accuracy(based on retrieved_docs).\n",
    "\n",
    "    User Query:\n",
    "    {query}\n",
    "\n",
    "    Retrieved Documents:\n",
    "    {retrieved_docs}\n",
    "\n",
    "    Instructions:\n",
    "    - If `retrieved_docs` is empty, respond: \"There is not enough information to answer your question.\"\n",
    "    - If the provided documents do not adequately address the query, respond: \"Sorry, we don't have enough data to address your question.\"\n",
    "\n",
    "    Generate a refined and well-structured response to the original query based on the retrieved information.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = llm.predict(prompt)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradeHallucinations(documents: str, generation: str) -> str:\n",
    "    \"\"\"\n",
    "    Grades a student's answer based on given FACTS.\n",
    "    \n",
    "    - Score 1: The student's answer is grounded in the FACTS.\n",
    "    - Score 0: The student's answer contains hallucinated information.\n",
    "    \n",
    "    Returns \"0\" or \"1\" as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER.\n",
    "        \n",
    "        Grading Criteria:\n",
    "        1. Ensure the STUDENT ANSWER is grounded in the FACTS.\n",
    "        2. Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "        Score:\n",
    "        - \"1\" if the student's answer is based on the FACTS.\n",
    "        - \"0\" if the student's answer is not based on the FACTS.\n",
    "\n",
    "        NOTE: Only return \"0\" or \"1\", otherwise you will be highly penalized.\n",
    "\n",
    "        FACTS:\n",
    "        {documents}\n",
    "        \n",
    "        STUDENT ANSWER:\n",
    "        {generation}\n",
    "        \"\"\"\n",
    "    \n",
    "    return llm.predict(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    \"\"\"\n",
    "    Generates a response based on the query using a multi-step RAG pipeline.\n",
    "    The response is saved to a Markdown file if it passes the hallucination check.\n",
    "    \"\"\"\n",
    "    # Step 1: Restructure the query for better retrieval\n",
    "    refined_query = restructure_query(query)\n",
    "    \n",
    "    # Step 2: Retrieve relevant documents based on the refined query\n",
    "    relevant_docs = get_relevant_docs(refined_query)\n",
    "    \n",
    "    # Step 3: Refine the retrieved documents using an LLM\n",
    "    refined_docs = refine_documents_with_llm(query, relevant_docs)\n",
    "    \n",
    "    # Step 4: Generate the final response based on the refined documents\n",
    "    response = final_content_refinement_tool(query, refined_docs)\n",
    "    \n",
    "    # Step 5: Check if the response is based on the retrieved documents\n",
    "    if GradeHallucinations(refined_docs, response):\n",
    "        safe_query = re.sub(r'[<>:\"/\\\\|?*]', '_', query)\n",
    "        output_dir = \"./data/generated_output\"\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "        output_path = os.path.join(output_dir, f\"{safe_query}.md\")\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response)\n",
    "        print(f\"Success: Response saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"Error: The generated response is not factually grounded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Response saved to ./data/generated_output\\Does HDFC ERGO optima secure covers AYUSH treatment_.md\n"
     ]
    }
   ],
   "source": [
    "query = \"Does HDFC ERGO optima secure covers AYUSH treatment?\"\n",
    "generate_response(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search\n",
    "query = \"All details about CARE Supre OPD treatment\"\n",
    "docs = web_search_tool.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Care Supreme Must-look Features & Claims Settlement Ratio',\n",
       "  'url': 'https://1finance.co.in/product-scoring/health-insurance/care-supreme?gender=Male&age=46-50&family=self-&sum=10%20Lacs',\n",
       "  'content': '... Finance: Explore the essential must-look features of Care Supreme and delve into their claims settlement ratio ... Claim Settlement Ratio (CSR) - Number92.81%.',\n",
       "  'score': 0.91563576},\n",
       " {'title': 'Care Insurance Care Supreme Plan Features, Benefits, Review ...',\n",
       "  'url': 'https://www.beshak.org/insurance/health-insurance/best-health-insurance-plans/care-insurance-care-supreme/',\n",
       "  'content': 'Care Insurance Health Insurance Company has a claim settlement ratio of 90.50%. Network hospitals: Care Insurance Health Insurance Company has',\n",
       "  'score': 0.7613660444444444},\n",
       " {'title': 'Care Health Insurance UPDATED Review 2025 - YouTube',\n",
       "  'url': 'https://www.youtube.com/watch?v=gnJyocoSiMA',\n",
       "  'content': \"With a claim settlement ratio of 90% and updated operational metrics, Care ... Don't buy Care Supreme or Care advantage before watching\",\n",
       "  'score': 0.6232087088888889},\n",
       " {'title': 'Care Supreme vs Zuno (erstwhile Edelweiss) Health Insurance ...',\n",
       "  'url': 'https://joinditto.in/health-insurance/compare-plans/care-supreme-vs-health-insurance-platinum/',\n",
       "  'content': 'Care Health Insurance has a remarkable claim settlement ratio of 90% and a network of more than 11,400+ hospitals. Health Insurance Platinum meanwhile comes',\n",
       "  'score': 0.5275909999999999},\n",
       " {'title': 'Care Health Insurance Claim Settlement Ratio, Process, Form',\n",
       "  'url': 'https://www.policybazaar.com/insurance-companies/religare-health-insurance/claim-settlement-ratio/',\n",
       "  'content': 'Care Health Insurance has secured an impressive claim settlement ratio of 87.1% in FY 2021-22. This shows that the insurer has a consistent claim settlement',\n",
       "  'score': 0.42412855555555556}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
