{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Multiple PDF processing start ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "import faiss\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "# from langchain.agents import initialize_agent, Tool, AgentType\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['LLAMA_CLOUD_API_KEY'] = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"openai_api_key\"] = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 610b5674-d09e-4114-9c0e-d3db0e97d6f1\n",
      "..Started parsing the file under job_id 0e1df70f-f9de-4cad-9331-9d20432c6c51\n",
      "..Started parsing the file under job_id 72e6475d-b8ef-4c37-9a5a-a616e6b7edae\n",
      ".Started parsing the file under job_id 07428a06-c989-4d78-b4bc-9ea9d36cf8b3\n",
      "Started parsing the file under job_id c99969ed-0f4e-4c74-9b50-f48848a3511d\n",
      "..Started parsing the file under job_id 188a95ad-ddf7-40bf-b380-37d1f29d1271\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# 1. Process multiple PDFs and create restructured markdown files\n",
    "# --------------------------------------------------\n",
    "# Adjust the pattern if your PDFs are named as policy-a-plan1.pdf, policy-a-plan2.pdf, etc.\n",
    "pdf_files = glob.glob('./data/input_data/pdf/policy-*-plan-*.pdf')\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    # Use LlamaParse to convert PDF to markdown with a refinement prompt.\n",
    "    documents = LlamaParse(\n",
    "        result_type=\"markdown\",\n",
    "        user_prompt=\"\"\"Streamline the policy document for clarity and conciseness while retaining all essential details. Remove redundancy without compromising key policies, coverage, or regulatory information. Ensure the final document is precise and well-structured\"\"\"\n",
    "    ).load_data(pdf_file)\n",
    "    \n",
    "    # Create an output filename based on the PDF name.\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "    output_md_path = f'./data/input_data/markdown/{base_name}-restructured.md'\n",
    "    \n",
    "    with open(output_md_path, 'w') as f:\n",
    "        for doc in documents:\n",
    "            f.write(doc.text + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 2. Load all restructured markdown files and attach metadata\n",
    "# --------------------------------------------------\n",
    "md_files = glob.glob('./data/input_data/markdown/*-restructured.md')\n",
    "all_documents = []\n",
    "for md_file in md_files:\n",
    "    loader = UnstructuredMarkdownLoader(md_file)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        # Extract company and plan information from the filename.\n",
    "        # Assumes filename pattern: policy-<company>-plan<plan_number>-restructured.md\n",
    "        match = re.search(r'policy-([a-zA-Z_]+)-plan-([a-zA-Z_]+)', md_file, re.IGNORECASE)\n",
    "        if match:\n",
    "            doc.metadata[\"company\"] = match.group(1).upper()\n",
    "            doc.metadata[\"plan\"] = match.group(2)\n",
    "        else:\n",
    "            doc.metadata[\"source_file\"] = md_file\n",
    "    all_documents.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 3. Split the loaded documents into chunks\n",
    "# --------------------------------------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs_split = text_splitter.split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the chunks have correct metadata\n",
    "# docs_split[600].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 4. Create embeddings using embeddings from Azure OpenAI\n",
    "# --------------------------------------------------\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",  # Your Azure deployment name for embeddings\n",
    "    openai_api_version=\"2024-02-01\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OpenAI embedding model\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # You can choose a different model if needed\n",
    "    openai_api_key=os.environ[\"openai_api_key\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents=docs_split,  # List of document chunks\n",
    "    embedding=embedding_model, \n",
    "    persist_directory=\"./data/vector_data/chroma_db\"  # Path to persist data\n",
    ")\n",
    "\n",
    "# Persist the data\n",
    "vector_store.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Multiple PDF processing End ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the vector store\n",
    "# vector_store = FAISS.from_documents(docs_split, embedding_model)\n",
    "\n",
    "# # Save FAISS index and document store\n",
    "# faiss.write_index(vector_store.index, \"./data/vector_data/faiss_index\")\n",
    "# with open(\"./data/vector_data/faiss_documents.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vector_store.docstore._dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
